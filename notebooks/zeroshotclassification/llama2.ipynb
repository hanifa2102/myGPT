{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36606bbd",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "781e01a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9930/3509738684.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628e8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc4c70f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/mohdhanifa/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f589b",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df5245",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    " - https://huggingface.co/blog/llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22118f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model=\"meta-llama/Llama-2-13b-chat-hf\"):\n",
    "    gen = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\")\n",
    "    sequences = gen(\n",
    "                    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "                    do_sample=True,\n",
    "                    top_k=10,\n",
    "                    num_return_sequences=1,\n",
    "                #     eos_token_id=tokenizer.eos_token_id,\n",
    "                    max_length=200,\n",
    "                    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")\n",
    "    return gen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d54785bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt_template():\n",
    "    prompt='''You must classify the products delimited by triple backticks.:\n",
    "Provide the output as a JSON string with the following keys only:\n",
    "[{\n",
    "    \"index\":,\n",
    "    \"Color\":,\n",
    "},\n",
    "{\n",
    "    \"index\":,\n",
    "    \"Color\":,\n",
    "}\n",
    "].\n",
    "```{\"fruits\":[{\"name\":\"Apple\",\"index\":0},{\"name\":\"Banana\",\"index\":1},{\"name\":\"Orange\",\"index\":2},{\"name\":\"Grapes\",\"index\":3},{\"name\":\"Strawberry\",\"index\":4}]}```'''\n",
    "    system_message = \"You are a helpful, respectful and honest assistant who talks and returns in JSON data.No AI introduction, No AI analysis.\"\n",
    "    prompt_template=f'''[INST] <<SYS>>\n",
    "    {system_message}\n",
    "    <</SYS>>\n",
    "\n",
    "    {prompt} [/INST]'''\n",
    "    print(f'prompt_template :\\t\\n {prompt_template}')\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc74e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24048212286c47d9b4390254c300ed43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen=get_model(\"meta-llama/Llama-2-70b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708adc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=gen_prompt_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf2ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = gen(\n",
    "#    prompt_template,\n",
    "#     do_sample=True,\n",
    "#     top_k=10,\n",
    "#     num_return_sequences=1,\n",
    "# #     eos_token_id=tokenizer.eos_token_id,\n",
    "#     max_length=2000,\n",
    "# )\n",
    "# for seq in sequences:\n",
    "#     print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264268b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# # import transformers\n",
    "# import torch\n",
    "\n",
    "# model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# gen = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0091225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aad19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(classifier(\n",
    "#     [\"The final marketing rule will not require an adviser to monitor the eligibility of compensated promoters on a continuous basis, as one commenter suggested\",\n",
    "#      \"Advisers could likely take a similar approach to monitoring promoters as they take in monitoring their own supervised persons, \\\n",
    "#      though advisers may assess the eligibility of their supervised persons more frequently in light of their obligations to report promptly certain disciplinary events on Form ADV.\",\n",
    "#     \"The marketing plan was a great success\",\"The whole world cup was negative\"],\n",
    "#     candidate_labels=[\"positive\",\"negative\",\"neutral\"],multi_label=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483008c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
